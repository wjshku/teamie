from fastapi import FastAPI, UploadFile, File, HTTPException, Form, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse, Response
from starlette.staticfiles import StaticFiles as StarletteStaticFiles
import os
import json
import logging
import traceback
from typing import Optional, List
from datetime import datetime, timedelta
from dotenv import load_dotenv
import tiktoken

from models import *
from data_manager import DataManager
from ai_analyzer import AIAnalyzer

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('app.log', encoding='utf-8')  # è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)
logger = logging.getLogger(__name__)

app = FastAPI(title="Teamie API", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥è®¾ç½®å…·ä½“çš„åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–ç»„ä»¶
data_dir = os.getenv("DATA_DIR", "data")
data_manager = DataManager(data_dir)
ai_analyzer = AIAnalyzer()

# ä» config æ¨¡å—å¯¼å…¥æ¨¡å‹é…ç½®
from config import MODEL_CONFIG, get_current_model, get_available_models

# å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
CURRENT_MODEL = get_current_model()

def get_model_config(model_name: str = None) -> dict:
    """è·å–æ¨¡å‹é…ç½®"""
    if model_name is None:
        model_name = CURRENT_MODEL
    
    if model_name not in MODEL_CONFIG:
        logger.warning(f"æ¨¡å‹ {model_name} ä¸åœ¨é…ç½®è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        model_name = get_current_model()
    
    return MODEL_CONFIG[model_name]

def get_tokens_per_second(model_name: str = None) -> float:
    """è·å–æ¨¡å‹çš„å¤„ç†é€Ÿåº¦ï¼ˆtokens/ç§’ï¼‰"""
    config = get_model_config(model_name)
    return config["tokens_per_second"]

def estimate_prompt_tokens(file_contents: list, previous_week_plan: Optional[list] = None) -> int:
    """ä¼°ç®—å‘é€ç»™AIçš„promptçš„tokenæ•°é‡"""
    estimated_tokens = 0

    # ç³»ç»Ÿprompté•¿åº¦ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
    estimated_tokens += 1000  # ç³»ç»Ÿpromptå¤§çº¦1000 tokens

    # æ–‡ä»¶å†…å®¹é•¿åº¦
    total_content_length = sum(len(item['content']) for item in file_contents)
    # ç®€å•ä¼°ç®—ï¼š1å­—ç¬¦ â‰ˆ 0.3 tokensï¼ˆä¸­æ–‡å’Œè‹±æ–‡æ··åˆï¼‰
    estimated_tokens += int(total_content_length * 0.3)

    # å¦‚æœæœ‰ä¸Šå‘¨è®¡åˆ’ï¼Œå¢åŠ é¢å¤–tokens
    if previous_week_plan and len(previous_week_plan) > 0:
        estimated_tokens += len(previous_week_plan) * 50  # æ¯é¡¹è®¡åˆ’å¤§çº¦50 tokens

    return estimated_tokens

# åˆå§‹åŒ– tiktoken ç¼–ç å™¨ï¼ˆç”¨äºè®¡ç®— token æ•°é‡ï¼‰
def get_token_count(text: str, model_name: str = None) -> int:
    """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    try:
        config = get_model_config(model_name)
        encoding_model = config.get("encoding_model", "gpt-4")
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹çš„ç¼–ç å™¨
        encoding = tiktoken.encoding_for_model(encoding_model)
        return len(encoding.encode(text))
    except Exception as e:
        logger.warning(f"Token è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—: {str(e)}")
        # å¦‚æœ tiktoken å¤±è´¥ï¼Œä½¿ç”¨ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰æˆ– 1.5 å­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰
        # è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼š1 token = 2 å­—ç¬¦
        return len(text) // 2

@app.get("/api/")
async def api_root():
    """APIå¥åº·æ£€æŸ¥"""
    return {"message": "Teamie API is running", "version": "1.0.0"}

async def process_files_in_background(project_id: str, file_contents: list, week_start_date: str):
    """åå°å¤„ç†æ–‡ä»¶ä¿å­˜å’ŒAIåˆ†æ"""
    try:
        logger.info(f"åå°ä»»åŠ¡å¼€å§‹: é¡¹ç›® {project_id}")
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶å†…å®¹åˆ°dataç›®å½•ï¼ˆä¿æŒåŸæ ¼å¼å’Œæ–‡ä»¶å¤¹ç»“æ„ï¼‰
        logger.info("æ­£åœ¨ä¿å­˜åŸå§‹æ–‡ä»¶...")
        for file_item in file_contents:
            filename = file_item['filename']
            content = file_item['content']
            relative_path = file_item.get('relative_path')  # è·å–ç›¸å¯¹è·¯å¾„
            data_manager.save_file_content(project_id, content, filename, week=1, relative_path=relative_path)
            logger.info(f"æ–‡ä»¶ {filename} ä¿å­˜æˆåŠŸ (è·¯å¾„: {relative_path or 'æ ¹ç›®å½•'})")
        logger.info("æ‰€æœ‰åŸå§‹æ–‡ä»¶ä¿å­˜å®Œæˆ")

        # åˆ†ææ–‡ä»¶å†…å®¹ç”ŸæˆæŠ¥å‘Š
        logger.info("æ­£åœ¨åˆ†ææ–‡ä»¶å†…å®¹...")
        analysis_result = ai_analyzer.analyze_html_contents(project_id, file_contents)
        week_data = analysis_result['week_data']
        logger.info("æ–‡ä»¶å†…å®¹åˆ†æå®Œæˆ")
        logger.info(f"AIåˆ†æç»Ÿè®¡: prompté•¿åº¦={analysis_result['prompt_length']}, prompt_tokens={analysis_result['prompt_tokens']}, completion_tokens={analysis_result['completion_tokens']}, total_tokens={analysis_result['total_tokens']}")

        # è®¾ç½®å‘¨æœŸé—´éš”ï¼ˆåŸºäºç”¨æˆ·é€‰æ‹©çš„æ—¥æœŸï¼‰
        if week_start_date:
            start_date = datetime.fromisoformat(week_start_date)
            end_date = start_date + timedelta(days=6)  # å‘¨ä¸€åˆ°å‘¨æ—¥
            week_data.week_period = f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
            logger.info(f"è®¾ç½®å‘¨æœŸé—´éš”: {week_data.week_period}")

        # ä¿å­˜ç¬¬ä¸€å‘¨æ•°æ®
        logger.info("æ­£åœ¨ä¿å­˜ç¬¬ä¸€å‘¨æ•°æ®...")
        data_manager.update_week_data(project_id, 1, week_data)
        logger.info("ç¬¬ä¸€å‘¨æ•°æ®ä¿å­˜æˆåŠŸ")

        logger.info(f"åå°ä»»åŠ¡å®Œæˆ: é¡¹ç›® {project_id} åˆ†æå®Œæˆ")
    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡å¤±è´¥: é¡¹ç›® {project_id}, é”™è¯¯: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

@app.post("/api/upload", response_model=UploadResponse)
async def upload_files(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    project_name: str = Form(...),
    week_start_date: str = Form(...),
    file_paths: List[str] = Form(None)  # å¯é€‰çš„è·¯å¾„ä¿¡æ¯åˆ—è¡¨
):
    """ä¸Šä¼ å¹¶åˆ†ææ–‡ä»¶ï¼ˆæ”¯æŒ html/txt/md æ ¼å¼ï¼Œæ”¯æŒå¤šæ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼‰"""
    logger.info(f"å¼€å§‹ä¸Šä¼  {len(files)} ä¸ªæ–‡ä»¶, é¡¹ç›®åç§°: {project_name}")

    try:
        # è¿‡æ»¤å¹¶è¯»å–æ”¯æŒçš„æ–‡ä»¶å†…å®¹ï¼ˆhtml/txt/mdï¼‰
        file_contents = []
        processed_files = []
        supported_extensions = ('.html', '.htm', '.txt', '.md')

        def get_file_extension(filename: str) -> str:
            """è·å–æ–‡ä»¶æ‰©å±•åï¼ˆä¸åŒ…å«ç‚¹ï¼‰"""
            if not filename:
                return ''
            # å¤„ç†è·¯å¾„åˆ†éš”ç¬¦
            filename = filename.replace('\\', '/')
            # è·å–æ–‡ä»¶åéƒ¨åˆ†
            basename = filename.split('/')[-1]
            # è·å–æ‰©å±•å
            if '.' in basename:
                return '.' + basename.split('.')[-1].lower()
            return ''

        # å¤„ç†æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        path_map = {}
        if file_paths:
            for i, path in enumerate(file_paths):
                if path:
                    path_map[i] = path
        
        file_index = 0
        for file in files:
            logger.info(f"æ£€æŸ¥æ–‡ä»¶: filename={file.filename}, content_type={file.content_type}")
            file_ext = get_file_extension(file.filename)
            
            if file.filename and file_ext in supported_extensions:
                logger.info(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file.filename} (æ‰©å±•å: {file_ext})")
                try:
                    content = await file.read()
                    file_content = content.decode('utf-8')

                    # æ¸…ç†æ–‡ä»¶å
                    cleaned_filename = data_manager._clean_filename(file.filename)
                    
                    # è·å–ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
                    relative_path = path_map.get(file_index)

                    file_contents.append({
                        'filename': cleaned_filename,
                        'content': file_content,
                        'relative_path': relative_path
                    })
                    processed_files.append(cleaned_filename)
                    logger.info(f"æ–‡ä»¶ {cleaned_filename} è¯»å–æˆåŠŸï¼Œå¤§å°: {len(file_content)} å­—ç¬¦ (è·¯å¾„: {relative_path or 'æ ¹ç›®å½•'})")
                    file_index += 1
                except UnicodeDecodeError as e:
                    logger.warning(f"æ–‡ä»¶ {file.filename} è§£ç å¤±è´¥ï¼Œè·³è¿‡: {str(e)}")
            else:
                logger.warning(f"è·³è¿‡æ–‡ä»¶: {file.filename} (æ‰©å±•å: {file_ext}, ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æˆ–æ–‡ä»¶åä¸ºç©º)")
                if file.filename:
                    file_index += 1

        if not file_contents:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ï¼ˆæ”¯æŒ html/txt/md æ ¼å¼ï¼‰")

        # è·å–å½“å‰æ¨¡å‹çš„é…ç½®
        tokens_per_second = get_tokens_per_second()
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {CURRENT_MODEL}, å¤„ç†é€Ÿåº¦: {tokens_per_second} tokens/ç§’")

        # è®¡ç®— token æ•°é‡
        total_token_count = 0
        for file_item in file_contents:
            content = file_item['content']
            token_count = get_token_count(content, CURRENT_MODEL)
            total_token_count += token_count
            logger.debug(f"æ–‡ä»¶ {file_item['filename']} token æ•°é‡: {token_count}")

        # è®¡ç®—é¢„è®¡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        estimated_time = total_token_count / tokens_per_second

        # æ–‡ä»¶æ•°é‡
        file_count = len(file_contents)

        logger.info(f"ä¸Šä¼ æ–‡ä»¶æ€»æ•°: {len(files)}, ç¬¦åˆè¦æ±‚çš„æ–‡ä»¶æ•°é‡: {file_count}, æ€» token æ•°é‡: {total_token_count}ï¼Œé¢„è®¡å¤„ç†æ—¶é—´: {estimated_time:.2f} ç§’")

        # åˆ›å»ºé¡¹ç›®ï¼ˆéœ€è¦ç«‹å³åˆ›å»ºï¼Œä»¥ä¾¿è¿”å› project_idï¼‰
        logger.info(f"æ­£åœ¨åˆ›å»ºé¡¹ç›®: {project_name}")
        project_id = data_manager.create_project(project_name)
        logger.info(f"é¡¹ç›®åˆ›å»ºæˆåŠŸï¼ŒID: {project_id}")

        # ç«‹å³è¿”å›å“åº”ï¼Œè®©å‰ç«¯æ˜¾ç¤º token å’Œé¢„è®¡æ—¶é—´
        logger.info(f"ç«‹å³è¿”å›å“åº”ï¼Œé¡¹ç›®ID: {project_id}, æ–‡ä»¶æ•°é‡: {file_count}, Token: {total_token_count}, é¢„è®¡æ—¶é—´: {estimated_time:.2f}ç§’")
        
        # æ·»åŠ åå°ä»»åŠ¡ï¼šä¿å­˜æ–‡ä»¶ã€AIåˆ†æã€ä¿å­˜æ•°æ®
        background_tasks.add_task(
            process_files_in_background,
            project_id=project_id,
            file_contents=file_contents,
            week_start_date=week_start_date
        )

        return UploadResponse(
            success=True,
            message=f"æˆåŠŸä¸Šä¼  {file_count} ä¸ªæ–‡ä»¶ï¼Œæ­£åœ¨åå°åˆ†æä¸­...",
            project_id=project_id,
            file_count=file_count,
            token_count=total_token_count,
            estimated_time_seconds=estimated_time
        )

    except Exception as e:
        logger.error(f"ä¸Šä¼ å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"æ–‡ä»¶ä¿¡æ¯: {[f.filename for f in files]}, é¡¹ç›®åç§°: {project_name}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.get("/api/projects", response_model=List[ProjectSummary])
async def get_projects():
    """è·å–æ‰€æœ‰é¡¹ç›®åˆ—è¡¨"""
    logger.info("æ­£åœ¨è·å–æ‰€æœ‰é¡¹ç›®åˆ—è¡¨")
    try:
        projects = data_manager.get_all_projects()
        logger.info(f"æˆåŠŸè·å– {len(projects)} ä¸ªé¡¹ç›®")
        return projects
    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®åˆ—è¡¨å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–é¡¹ç›®åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/projects/{project_id}/week/{week}")
async def get_week_report(project_id: str, week: int):
    """è·å–æŒ‡å®šé¡¹ç›®çš„å‘¨æŠ¥"""
    logger.info(f"æ­£åœ¨è·å–é¡¹ç›® {project_id} çš„ç¬¬ {week} å‘¨å‘¨æŠ¥")
    try:
        week_data = data_manager.get_week_data(project_id, week)
        if not week_data:
            logger.warning(f"é¡¹ç›® {project_id} çš„ç¬¬ {week} å‘¨å‘¨æŠ¥ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="å‘¨æŠ¥ä¸å­˜åœ¨")

        logger.info(f"æˆåŠŸè·å–é¡¹ç›® {project_id} çš„ç¬¬ {week} å‘¨å‘¨æŠ¥")
        return week_data.dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å‘¨æŠ¥å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}, å‘¨æ•°: {week}")
        raise HTTPException(status_code=500, detail=f"è·å–å‘¨æŠ¥å¤±è´¥: {str(e)}")

@app.post("/api/projects/{project_id}/week/{week}/update-plan")
async def update_week_plan(project_id: str, week: int, plan_data: Dict[str, Any]):
    """æ›´æ–°å‘¨æŠ¥çš„ä¸‹å‘¨è®¡åˆ’"""
    logger.info(f"æ­£åœ¨æ›´æ–°é¡¹ç›® {project_id} ç¬¬ {week} å‘¨çš„ä¸‹å‘¨è®¡åˆ’")
    logger.debug(f"è®¡åˆ’æ•°æ®: {plan_data}")

    try:
        # è·å–ç°æœ‰å‘¨æ•°æ®
        week_data = data_manager.get_week_data(project_id, week)
        if not week_data:
            logger.info(f"é¡¹ç›® {project_id} ç¬¬ {week} å‘¨æ•°æ®ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„WeekData")
            week_data = WeekData()

        # æ›´æ–°ä¸‹å‘¨è®¡åˆ’
        if "next_week_plan" in plan_data:
            logger.info("æ­£åœ¨ä½¿ç”¨AIåˆ†æå™¨æ›´æ–°ä¸‹å‘¨è®¡åˆ’")
            week_data = ai_analyzer.update_week_data_with_plan(week_data, plan_data["next_week_plan"])
            logger.info("ä¸‹å‘¨è®¡åˆ’æ›´æ–°å®Œæˆ")

        # ä¿å­˜æ›´æ–°
        logger.info("æ­£åœ¨ä¿å­˜æ›´æ–°åçš„å‘¨æ•°æ®")
        success = data_manager.update_week_data(project_id, week, week_data)

        if not success:
            logger.error(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨ï¼Œæ— æ³•ä¿å­˜æ•°æ®")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        logger.info(f"é¡¹ç›® {project_id} ç¬¬ {week} å‘¨çš„ä¸‹å‘¨è®¡åˆ’æ›´æ–°æˆåŠŸ")
        return {"success": True, "message": "ä¸‹å‘¨è®¡åˆ’æ›´æ–°æˆåŠŸ"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°ä¸‹å‘¨è®¡åˆ’å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}, å‘¨æ•°: {week}, è®¡åˆ’æ•°æ®: {plan_data}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")

@app.put("/api/projects/{project_id}/week/{week}")
async def update_week_report(project_id: str, week: int, week_data: WeekData):
    """æ›´æ–°å®Œæ•´å‘¨æŠ¥æ•°æ®"""
    logger.info(f"æ­£åœ¨æ›´æ–°é¡¹ç›® {project_id} ç¬¬ {week} å‘¨çš„å®Œæ•´å‘¨æŠ¥æ•°æ®")
    logger.debug(f"å‘¨æŠ¥æ•°æ®ç±»å‹: {type(week_data)}, å­—æ®µ: {week_data.__dict__.keys() if hasattr(week_data, '__dict__') else 'N/A'}")

    try:
        logger.info("æ­£åœ¨ä¿å­˜å‘¨æŠ¥æ•°æ®")
        success = data_manager.update_week_data(project_id, week, week_data)

        if not success:
            logger.error(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°å‘¨æŠ¥")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        logger.info(f"é¡¹ç›® {project_id} ç¬¬ {week} å‘¨çš„å‘¨æŠ¥æ›´æ–°æˆåŠŸ")
        return {"success": True, "message": "å‘¨æŠ¥æ›´æ–°æˆåŠŸ"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°å‘¨æŠ¥å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}, å‘¨æ•°: {week}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")

@app.get("/api/projects/{project_id}")
async def get_project_info(project_id: str):
    """è·å–é¡¹ç›®ä¿¡æ¯"""
    logger.info(f"æ­£åœ¨è·å–é¡¹ç›® {project_id} çš„ä¿¡æ¯")
    try:
        project = data_manager.get_project(project_id)
        if not project:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        result = {
            "id": project.id,
            "name": project.name,
            "created_at": project.created_at,
            "updated_at": project.updated_at,
            "total_weeks": len(project.weeks),
            "current_week": max(project.weeks.keys()) if project.weeks else 1
        }
        logger.info(f"æˆåŠŸè·å–é¡¹ç›® {project_id} ä¿¡æ¯: {len(project.weeks)} å‘¨æ•°æ®")
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®ä¿¡æ¯å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}")
        raise HTTPException(status_code=500, detail=f"è·å–é¡¹ç›®ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.put("/api/projects/{project_id}/status")
async def update_project_status(project_id: str, request: dict):
    """æ›´æ–°é¡¹ç›®çŠ¶æ€"""
    status = request.get("status", "").strip()
    if not status:
        raise HTTPException(status_code=400, detail="çŠ¶æ€ä¸èƒ½ä¸ºç©º")

    logger.info(f"æ­£åœ¨æ›´æ–°é¡¹ç›® {project_id} çš„çŠ¶æ€ä¸º: {status}")
    try:
        success = data_manager.update_project_status(project_id, status)
        if not success:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°çŠ¶æ€")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        logger.info(f"é¡¹ç›® {project_id} çŠ¶æ€æ›´æ–°æˆåŠŸ")
        return {"success": True, "message": "çŠ¶æ€æ›´æ–°æˆåŠŸ"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°é¡¹ç›®çŠ¶æ€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}, æ–°çŠ¶æ€: {status}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°çŠ¶æ€å¤±è´¥: {str(e)}")

@app.delete("/api/projects/{project_id}")
async def delete_project(project_id: str):
    """åˆ é™¤é¡¹ç›®"""
    logger.info(f"æ­£åœ¨åˆ é™¤é¡¹ç›® {project_id}")
    try:
        success = data_manager.delete_project(project_id)
        if not success:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        logger.info(f"é¡¹ç›® {project_id} åˆ é™¤æˆåŠŸ")
        return {"success": True, "message": "é¡¹ç›®åˆ é™¤æˆåŠŸ"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤é¡¹ç›®å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤é¡¹ç›®å¤±è´¥: {str(e)}")

@app.get("/api/projects/{project_id}/week/{week}/files")
def get_project_week_files(project_id: str, week: int):
    """è·å–é¡¹ç›®æŒ‡å®šå‘¨çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨"""
    logger.info(f"è·å–é¡¹ç›® {project_id} ç¬¬ {week} å‘¨çš„æ–‡ä»¶åˆ—è¡¨")

    try:
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å­˜åœ¨
        project = data_manager.get_project(project_id)
        if not project:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = data_manager.get_files(project_id, week)
        logger.info(f"é¡¹ç›® {project_id} ç¬¬ {week} å‘¨æœ‰ {len(files)} ä¸ªæ–‡ä»¶")

        return {"files": files}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/projects/{project_id}/week/{week}/files/{filename:path}")
def get_project_week_file_content(project_id: str, week: int, filename: str):
    """è·å–é¡¹ç›®æŒ‡å®šå‘¨çš„å•ä¸ªæ–‡ä»¶å†…å®¹"""
    logger.info(f"è·å–é¡¹ç›® {project_id} ç¬¬ {week} å‘¨çš„æ–‡ä»¶ {filename} å†…å®¹")

    try:
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å­˜åœ¨
        project = data_manager.get_project(project_id)
        if not project:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        # è·å–æ–‡ä»¶å†…å®¹
        content = data_manager.get_file_content_by_name(project_id, week, filename)
        if content is None:
            logger.warning(f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åè®¾ç½® media_type
        filename_lower = filename.lower()
        if filename_lower.endswith(('.html', '.htm')):
            media_type = "text/html"
        elif filename_lower.endswith('.md'):
            media_type = "text/markdown"
        elif filename_lower.endswith('.txt'):
            media_type = "text/plain"
        else:
            media_type = "text/plain"

        logger.info(f"æˆåŠŸè·å–æ–‡ä»¶ {filename} å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
        return Response(content=content, media_type=media_type)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")

@app.post("/api/projects/{project_id}/analyze-next-week")
async def analyze_next_week(
    request: Request,
    project_id: str,
    html_content: str = Form(None),
    file_paths: List[str] = Form(None),  # æ–‡ä»¶è·¯å¾„ä¿¡æ¯åˆ—è¡¨
    update_current_week: str = Form("false"),  # æ˜¯å¦æ›´æ–°å½“å‰å‘¨ï¼ˆè€Œä¸æ˜¯åˆ›å»ºæ–°å‘¨ï¼‰
    week_start_date: str = Form(None),  # å‘¨å¼€å§‹æ—¥æœŸ
    files: List[UploadFile] = File(None)
):
    """åˆ†ææ–°ä¸€å‘¨çš„è¿›å±•ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼Œä¿æŒæ–‡ä»¶å¤¹ç»“æ„ï¼‰"""
    logger.info(f"å¼€å§‹åˆ†æé¡¹ç›® {project_id} çš„æ–°ä¸€å‘¨è¿›å±•, week_start_date: {repr(week_start_date)}")

    try:
        # è·å–ä¸Šä¸€å‘¨çš„æ•°æ®ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        logger.info(f"æ­£åœ¨è·å–é¡¹ç›® {project_id} çš„ä¿¡æ¯")
        project = data_manager.get_project(project_id)
        if not project:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        current_week = max(project.weeks.keys()) if project.weeks else 0
        previous_week_plan = None

        # åˆ¤æ–­æ˜¯æ›´æ–°å½“å‰å‘¨è¿˜æ˜¯åˆ›å»ºæ–°å‘¨
        is_update_current = update_current_week.lower() == "true"
        target_week = current_week if is_update_current else (current_week + 1)

        if current_week > 0 and project.weeks.get(current_week):
            previous_week_plan = project.weeks[current_week].next_week_plan
            logger.info(f"æ‰¾åˆ°ä¸Šä¸€å‘¨({current_week})çš„è®¡åˆ’æ•°æ®")

        # å¦‚æœæ˜¯æ›´æ–°å½“å‰å‘¨ï¼Œéœ€è¦è·å–å·²æœ‰çš„æ–‡ä»¶å†…å®¹
        existing_file_contents = []
        if is_update_current and current_week > 0:
            logger.info(f"æ›´æ–°å½“å‰å‘¨({current_week})ï¼Œè·å–å·²æœ‰æ–‡ä»¶...")
            # è·å–å½“å‰å‘¨çš„æ‰€æœ‰æ–‡ä»¶
            existing_files = data_manager.get_files(project_id, current_week)
            for file_path in existing_files:
                content = data_manager.get_file_content_by_name(project_id, current_week, file_path)
                if content:
                    existing_file_contents.append({
                        'filename': file_path.split('/')[-1],
                        'content': content,
                        'relative_path': file_path
                    })
            logger.info(f"æ‰¾åˆ° {len(existing_file_contents)} ä¸ªå·²æœ‰æ–‡ä»¶")

        # å¤„ç†æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        path_map = {}
        if file_paths:
            for i, path in enumerate(file_paths):
                if path:
                    path_map[i] = path

        # å¤„ç†è¾“å…¥ï¼šæ”¯æŒå•æ–‡ä»¶å­—ç¬¦ä¸²æˆ–å¤šæ–‡ä»¶ä¸Šä¼ 
        file_contents = existing_file_contents.copy()  # å…ˆå¤åˆ¶å·²æœ‰æ–‡ä»¶
        new_week = target_week
        supported_extensions = ('.html', '.htm', '.txt', '.md')

        file_index = 0
        if files and len(files) > 0:
            # å¤šæ–‡ä»¶æ¨¡å¼
            logger.info(f"å¤„ç†å¤šæ–‡ä»¶è¾“å…¥: {len(files)} ä¸ªæ–‡ä»¶")
            for file in files:
                if file.filename and any(file.filename.lower().endswith(ext) for ext in supported_extensions):
                    logger.info(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file.filename}")
                    content = await file.read()
                    file_content_data = content.decode('utf-8')
                    
                    # æ¸…ç†æ–‡ä»¶å
                    cleaned_filename = data_manager._clean_filename(file.filename)
                    
                    # è·å–ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
                    relative_path = path_map.get(file_index)
                    
                    file_contents.append({
                        'filename': cleaned_filename,
                        'content': file_content_data,
                        'relative_path': relative_path
                    })
                    logger.info(f"æ–‡ä»¶ {cleaned_filename} è¯»å–æˆåŠŸï¼Œå¤§å°: {len(file_content_data)} å­—ç¬¦ (è·¯å¾„: {relative_path or 'æ ¹ç›®å½•'})")

                    # ä¿å­˜åˆ°æ–°ä¸€å‘¨çš„ç›®å½•ï¼ˆä¿æŒæ–‡ä»¶å¤¹ç»“æ„ï¼‰
                    data_manager.save_file_content(project_id, file_content_data, cleaned_filename, week=new_week, relative_path=relative_path)
                    file_index += 1
        elif html_content:
            # å•æ–‡ä»¶æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            logger.info("å¤„ç†å•æ–‡ä»¶å­—ç¬¦ä¸²è¾“å…¥")
            logger.debug(f"æ–‡ä»¶å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
            file_contents.append({
                'filename': 'content.html',
                'content': html_content,
                'relative_path': None
            })
            # ä¿å­˜åˆ°æ–°ä¸€å‘¨çš„ç›®å½•
            data_manager.save_file_content(project_id, html_content, 'content.html', week=new_week)
        else:
            raise HTTPException(status_code=400, detail="æœªæä¾›æœ‰æ•ˆçš„æ–‡ä»¶å†…å®¹")

        if not file_contents:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ï¼ˆæ”¯æŒ html/txt/md æ ¼å¼ï¼‰")

        logger.info(f"å…±å¤„ç† {len(file_contents)} ä¸ªæ–‡ä»¶ï¼ˆå…¶ä¸­ {len(existing_file_contents)} ä¸ªå·²æœ‰æ–‡ä»¶ï¼Œ{len(file_contents) - len(existing_file_contents)} ä¸ªæ–°æ–‡ä»¶ï¼‰")

        # åœ¨è°ƒç”¨AIå‰ä¼°ç®—tokenæ•°é‡
        estimated_prompt_tokens = estimate_prompt_tokens(file_contents, previous_week_plan)
        estimated_time_seconds = estimated_prompt_tokens / 600  # å‡è®¾600 tokens/s

        logger.info(f"ä¼°ç®—çš„prompt tokens: {estimated_prompt_tokens}, é¢„è®¡å¤„ç†æ—¶é—´: {estimated_time_seconds:.2f}ç§’")

        # è·å–ç°æœ‰æ•°æ®ï¼ˆå¦‚æœæ˜¯æ›´æ–°å½“å‰å‘¨ï¼‰
        existing_week_data = None
        if is_update_current:
            existing_week_data = data_manager.get_week_data(project_id, new_week)
            logger.info(f"æ›´æ–°å½“å‰å‘¨ï¼Œè·å–ç°æœ‰æ•°æ®: week_period={repr(existing_week_data.week_period if existing_week_data else None)}")

        # åˆ†ææ•°æ®
        action_text = "æ›´æ–°" if is_update_current else "åˆ†æ"
        logger.info(f"æ­£åœ¨{action_text}ç¬¬ {new_week} å‘¨çš„æ•°æ®")
        analysis_result = ai_analyzer.analyze_html_contents(project_id, file_contents, previous_week_plan)
        week_data = analysis_result['week_data']
        logger.info(f"ç¬¬ {new_week} å‘¨æ•°æ®{action_text}å®Œæˆ")
        logger.info(f"AIåˆ†æç»Ÿè®¡: prompté•¿åº¦={analysis_result['prompt_length']}, prompt_tokens={analysis_result['prompt_tokens']}, completion_tokens={analysis_result['completion_tokens']}, total_tokens={analysis_result['total_tokens']}")

        # è®¾ç½®å‘¨æœŸé—´éš” - è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        logger.info(f"ğŸ” è®¾ç½®å‘¨æœŸé—´éš” - å¼€å§‹è°ƒè¯•:")
        logger.info(f"   is_update_current: {is_update_current}")
        logger.info(f"   existing_week_data å­˜åœ¨: {existing_week_data is not None}")
        if existing_week_data:
            logger.info(f"   existing_week_data.week_period: {repr(existing_week_data.week_period)}")
        logger.info(f"   week_start_date: {repr(week_start_date)}")
        logger.info(f"   new_week: {new_week}")

        if is_update_current and existing_week_data and existing_week_data.week_period:
            # æ›´æ–°å½“å‰å‘¨æ—¶ä¿ç•™ç°æœ‰çš„å‘¨æœŸé—´éš”
            week_data.week_period = existing_week_data.week_period
            logger.info(f"âœ… ä¿ç•™ç°æœ‰å‘¨æœŸé—´éš”: {week_data.week_period}")
        else:
            # åˆ›å»ºæ–°å‘¨æ—¶è®¾ç½®å‘¨æœŸé—´éš”ï¼ˆå‰ç«¯ä¿è¯ä¼ é€’æœ‰æ•ˆçš„æ—¥æœŸï¼‰
            logger.info(f"ğŸ”„ åˆ›å»ºæ–°å‘¨ï¼Œå‡†å¤‡è§£ææ—¥æœŸ: {repr(week_start_date)}")
            try:
                clean_date = week_start_date.strip()
                logger.info(f"   æ¸…ç†åçš„æ—¥æœŸ: {repr(clean_date)}")
                start_date = datetime.fromisoformat(clean_date)
                logger.info(f"   è§£æåçš„ start_date: {start_date}")
                end_date = start_date + timedelta(days=6)  # å‘¨ä¸€åˆ°å‘¨æ—¥
                logger.info(f"   è®¡ç®—åçš„ end_date: {end_date}")
                week_data.week_period = f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
                logger.info(f"âœ… è®¾ç½®ç¬¬ {new_week} å‘¨æœŸé—´éš”: {week_data.week_period}")
            except Exception as e:
                logger.error(f"âŒ æ—¥æœŸè§£æå¤±è´¥: week_start_date={repr(week_start_date)}, é”™è¯¯: {str(e)}")
                logger.error(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                week_data.week_period = None

        logger.info(f"ğŸ“Š è®¾ç½®å‘¨æœŸé—´éš” - æœ€ç»ˆç»“æœ: week_data.week_period = {repr(week_data.week_period)}")

        # å°†ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°å“åº”ä¸­ï¼ˆä½¿ç”¨å®é™…çš„ç»Ÿè®¡æ•°æ®ï¼‰
        result = {
            "success": True,
            "message": f"ç¬¬{new_week}å‘¨{action_text}å®Œæˆ",
            "week": new_week,
            "data": week_data.dict(),
            "week_period": week_data.week_period,
            "prompt_length": analysis_result['prompt_length'],
            "prompt_tokens": analysis_result['prompt_tokens'],
            "completion_tokens": analysis_result['completion_tokens'],
            "total_tokens": analysis_result['total_tokens']
        }

        # ä¿å­˜æ•°æ®
        logger.info(f"ğŸ’¾ ä¿å­˜å‰æ£€æŸ¥: week_data.week_period = {repr(week_data.week_period)}")
        data_manager.update_week_data(project_id, new_week, week_data)
        logger.info(f"é¡¹ç›® {project_id} ç¬¬ {new_week} å‘¨{action_text}å®Œæˆ")

        # éªŒè¯ä¿å­˜ç»“æœ
        saved_data = data_manager.get_week_data(project_id, new_week)
        if saved_data:
            logger.info(f"âœ… ä¿å­˜åéªŒè¯: saved_data.week_period = {repr(saved_data.week_period)}")
            if saved_data.week_period != week_data.week_period:
                logger.error(f"âŒ æ•°æ®ä¸ä¸€è‡´! å†…å­˜ä¸­: {repr(week_data.week_period)}, ä¿å­˜å: {repr(saved_data.week_period)}")
        else:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥! æ— æ³•è·å–ä¿å­˜çš„æ•°æ®")
        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ†ææ–°ä¸€å‘¨è¿›å±•å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logger.error(f"é¡¹ç›®ID: {project_id}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")

@app.delete("/api/projects/{project_id}/week/{week}")
async def delete_week_data(project_id: str, week: int):
    """åˆ é™¤æŒ‡å®šé¡¹ç›®çš„æŒ‡å®šå‘¨æ•°æ®"""
    logger.info(f"å¼€å§‹åˆ é™¤é¡¹ç›® {project_id} çš„ç¬¬ {week} å‘¨æ•°æ®")

    try:
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å­˜åœ¨
        project = data_manager.get_project(project_id)
        if not project:
            logger.warning(f"é¡¹ç›® {project_id} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")

        # æ£€æŸ¥å‘¨æ•°æ®æ˜¯å¦å­˜åœ¨
        if week not in project.weeks:
            logger.warning(f"é¡¹ç›® {project_id} çš„ç¬¬ {week} å‘¨æ•°æ®ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail=f"ç¬¬{week}å‘¨æ•°æ®ä¸å­˜åœ¨")

        # åˆ é™¤å‘¨æ•°æ®
        success = data_manager.delete_week_data(project_id, week)
        if not success:
            raise HTTPException(status_code=500, detail="åˆ é™¤å¤±è´¥")

        logger.info(f"é¡¹ç›® {project_id} çš„ç¬¬ {week} å‘¨æ•°æ®åˆ é™¤æˆåŠŸ")
        return {
            "success": True,
            "message": f"ç¬¬{week}å‘¨æ•°æ®å·²åˆ é™¤"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å‘¨æ•°æ®å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")

@app.get("/api/models")
async def get_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨å’Œå½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    try:
        available_models = get_available_models()
        current_model = get_current_model()
        return {
            "success": True,
            "available_models": available_models,
            "current_model": current_model
        }
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.post("/api/models/{model_id}")
async def set_model(model_id: str):
    """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    try:
        from config import set_current_model
        
        if model_id not in MODEL_CONFIG:
            raise HTTPException(status_code=400, detail=f"æ¨¡å‹ {model_id} ä¸åœ¨é…ç½®è¡¨ä¸­")
        
        set_current_model(model_id)
        
        # æ›´æ–°å…¨å±€å˜é‡
        global CURRENT_MODEL
        CURRENT_MODEL = model_id
        
        logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {model_id}")
        
        return {
            "success": True,
            "message": f"æ¨¡å‹å·²åˆ‡æ¢ä¸º {model_id}",
            "current_model": model_id
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è®¾ç½®æ¨¡å‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è®¾ç½®æ¨¡å‹å¤±è´¥: {str(e)}")

# æŒ‚è½½å‰ç«¯æ–‡ä»¶ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰APIè·¯ç”±ä¹‹åï¼‰
# åœ¨å¼€å‘ç¯å¢ƒä¸­ç¦ç”¨ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡éƒ½è·å–æœ€æ–°å†…å®¹
class NoCacheStaticFiles(StarletteStaticFiles):
    async def get_response(self, path, scope):
        response = await super().get_response(path, scope)
        # å¼€å‘ç¯å¢ƒç¦ç”¨ç¼“å­˜
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
        return response

# è‡ªåŠ¨æ£€æµ‹å‰ç«¯ç›®å½•è·¯å¾„ï¼ˆæ”¯æŒæœ¬åœ°å¼€å‘å’ŒDockeréƒ¨ç½²ï¼‰
import os
frontend_paths = ["../frontend", "./frontend", "/app/frontend"]
frontend_dir = None

for path in frontend_paths:
    if os.path.exists(path):
        frontend_dir = path
        break

if not frontend_dir:
    raise RuntimeError("Frontend directory not found. Checked paths: " + ", ".join(frontend_paths))

app.mount("/", NoCacheStaticFiles(directory=frontend_dir, html=True), name="frontend")

if __name__ == "__main__":
    import uvicorn
    import os
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host=host, port=port)
